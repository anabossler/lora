{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*1. Introducción *\n",
        "\n",
        "Low-Rank Adaptation, abreviado como LoRA, es una técnica propuesta en el campo del procesamiento de lenguaje natural y el aprendizaje automático. Su objetivo principal es abordar el desafío de adaptar modelos de lenguaje pre-entrenados, que son muy grandes y tienen una gran cantidad de parámetros, a tareas específicas o dominios sin incurrir en costes muy caros en términos de recursos computacionales y memoria GPU.\n",
        "\n",
        "La idea central detrás de LoRA es reducir significativamente el número de parámetros entrenables en un modelo al introducir matrices de descomposición de rango entrenables en cada capa de la arquitectura del modelo. Estas matrices de descomposición de rango son matrices más pequeñas que representan de manera eficiente la información de las capas originales del modelo, lo que reduce drásticamente la cantidad de memoria y recursos necesarios para entrenar y utilizar el modelo.\n",
        "\n",
        "LoRA es una técnica que permite mantener los pesos pre-entrenados de un modelo fijos y reemplazar una parte de los parámetros con matrices de descomposición de rango entrenables. Esto resulta en modelos más eficientes en términos de recursos y memoria, lo que facilita su adaptación a tareas específicas sin comprometer significativamente su rendimiento.\n",
        "\n",
        "Su base está definida en el paper Hu, Edward, et al. \"LORA: Low-Rank Adaptation of Large Language Models.\" Microsoft Corporation, Version 2, [https://arxiv.org/abs/2106.09685], (2021).\n",
        "\n",
        "En este trabajo se propone crear un modelo \"LoRA\" sencillo, para clasificar dígitos utilizando la base de datos MNIST. La idea es \"sobrecargar\" la red neuronal para que clasifique ineficientemente los dígitos, y, a partir del dígito peor clasificado, hacer fine-tuning utilizando LoRA en este dígito para saber si mejora el resultado de la clasificación.\n",
        "\n",
        "código adaptado de https://colab.research.google.com/drive/1iERDk94Jp0UErsPf7vXyPKeiM4ZJUQ-a?usp=sharing#scrollTo=WuK0lPwcB7Ia\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WJsjQy7JIrbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importamos las librerías necesarias\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "FDtUUbLlfDXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hacemos torch determinístico\n",
        "_ = torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "oakNS3zmg89J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# cargamos el dataset MNIST para train\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# creamos un dataloader para train\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "# cargamos MNIST para test\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
        "\n",
        "# definimos el device que vamos utilizar\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "E2RIogkznXKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos una red neuronal demasiado ineficiente para clasificar dígitos MNIST\n",
        "\n",
        "class desperdicio(nn.Module):\n",
        "    def __init__(self, hidden_size_1=1000, hidden_size_2=2000):\n",
        "        super(desperdicio,self).__init__()\n",
        "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
        "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = img.view(-1, 28*28)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "net = desperdicio().to(device)"
      ],
      "metadata": {
        "id": "teUtOdJhorP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, net, epochs=5, total_iterations_limit=None):\n",
        "    cross_el = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    total_iterations = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "\n",
        "        loss_sum = 0\n",
        "        num_iterations = 0\n",
        "\n",
        "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
        "        if total_iterations_limit is not None:\n",
        "            data_iterator.total = total_iterations_limit\n",
        "        for data in data_iterator:\n",
        "            num_iterations += 1\n",
        "            total_iterations += 1\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = net(x.view(-1, 28*28))\n",
        "            loss = cross_el(output, y)\n",
        "            loss_sum += loss.item()\n",
        "            avg_loss = loss_sum / num_iterations\n",
        "            data_iterator.set_postfix(loss=avg_loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
        "                return\n",
        "\n",
        "train(train_loader, net, epochs=1)"
      ],
      "metadata": {
        "id": "RkjjVJSBycGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f12c3ca-2e9b-4280-87cc-2518271216a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 6000/6000 [07:50<00:00, 12.75it/s, loss=0.236]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#salvamos los pesos originales para poder recuperarlos\n",
        "original_weights = {}\n",
        "for name, param in net.named_parameters():\n",
        "    original_weights[name] = param.clone().detach()"
      ],
      "metadata": {
        "id": "KVq2rBhRyhaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hacemos test en las clasificaciones donde vemos que se equivoca mucho en el dígito 9 en relación a los demás\n",
        "def test():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    wrong_counts = [0 for i in range(10)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, desc='Testing'):\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            output = net(x.view(-1, 784))\n",
        "            for idx, i in enumerate(output):\n",
        "                if torch.argmax(i) == y[idx]:\n",
        "                    correct +=1\n",
        "                else:\n",
        "                    wrong_counts[y[idx]] +=1\n",
        "                total +=1\n",
        "    print(f'Accuracy: {round(correct/total, 3)}')\n",
        "    for i in range(len(wrong_counts)):\n",
        "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6XIFfZSyjl9",
        "outputId": "3b97c605-d481-4a88-a4b0-23c2ffcbe81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:05<00:00, 168.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.954\n",
            "wrong counts for the digit 0: 31\n",
            "wrong counts for the digit 1: 17\n",
            "wrong counts for the digit 2: 46\n",
            "wrong counts for the digit 3: 74\n",
            "wrong counts for the digit 4: 29\n",
            "wrong counts for the digit 5: 7\n",
            "wrong counts for the digit 6: 36\n",
            "wrong counts for the digit 7: 80\n",
            "wrong counts for the digit 8: 25\n",
            "wrong counts for the digit 9: 116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# salvamos los parámetros originales\n",
        "total_parameters_original = 0\n",
        "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
        "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
        "print(f'Total number of parameters: {total_parameters_original:,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsjHsT9bynKE",
        "outputId": "6139db57-541c-4fe2-fbfb-afa7b22bbcc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
            "Total number of parameters: 2,807,010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La idea básica detrás de LoRA es mantener las matrices pre-entrenadas (es decir, los parámetros del modelo original) congeladas (en un estado fijo) y solo agregar un pequeño delta a la matriz original, que tiene menos parámetros que la matriz original.\n",
        "\n",
        "Por ejemplo, consideremos la matriz `W`, que podría ser los parámetros de una capa completamente conectada o una de las matrices del mecanismo de autoatención de un transformer:\n",
        "\n",
        "W_orig = W + DeltaW\n",
        "\n",
        "Si `W_orig` tuviera dimensiones `n x m` y simplemente inicializáramos una nueva matriz delta con las mismas dimensiones para afinarla, no habríamos ganado nada; más bien al contrario, habríamos duplicado los parámetros.\n",
        "\n",
        "El truco consiste en hacer que `DeltaW` sea menos \"dimensional\" que la matriz original, construyéndola mediante una multiplicación de matrices a partir de matrices de dimensiones más bajas `B` y `A`:\n",
        "\n",
        "DeltaW = B * A\n",
        "\n",
        "\n",
        "Primero definimos un rango `r`, que es significativamente menor que las dimensiones básicas de la matriz, `r << n` y `r << m`. La matriz `B` es de dimensiones `n x r` y la matriz `A` es de dimensiones `r x m`. Multiplicarlas produce una matriz con las mismas dimensiones que `W`, pero construida a partir de una cantidad mucho menor de parámetros.\n",
        "\n",
        "Queremos que nuestro delta sea cero al comienzo del entrenamiento, de modo que el fine tuning comience de la misma manera que el modelo original. Por lo tanto, `B` se inicializa como todo ceros y `A` se inicializa como valores aleatorios (generalmente distribuidos de manera normal).\n",
        "\n",
        "Además, en el paper de LoRA,la matriz delta está definida desde un  parámetro `alpha`:\n",
        "\n",
        "DeltaW = alpha * B * A\n",
        "\n",
        "Si configuramos el `alpha` con el primer `r` con el que intentamos y ajustamos la tasa de aprendizaje, podemos cambiar el parámetro `r` más tarde sin tener que volver a ajustar la tasa de aprendizaje (learning rate).\n",
        "\n",
        "En este sentido, la implementación de LoRA se realiza mediante la aplicación de una serie de transformaciones a las matrices de peso de una red neuronal. Estas transformaciones están diseñadas para preservar la información importante en los pesos mientras reducen su dimensionalidad. Las técnicas principales utilizadas en LoRA son:\n",
        "\n",
        "Transformación Lineal: Se aplica una transformación lineal a las matrices de peso para transformarlas en un espacio de menor dimensión.\n",
        "\n",
        "Aproximación de la Matriz de Peso: Las matrices de peso se aproximan utilizando una técnica de aproximación dispersa.\n",
        "\n",
        "Descomposición en Valores Singulares (SVD): La SVD se utiliza para factorizar las matrices de peso en el producto de tres matrices. Esta descomposición permite extraer los valores y vectores singulares más importantes, lo que conduce a una representación de bajo rango de los pesos.\n",
        "\n",
        "En LoRA, la SVD es utilizada para identificar y retener solo los componentes más significativos de las matrices de peso. Al factorizar una matriz de peso y conservar solo los valores singulares más grandes (y sus vectores correspondientes), se puede lograr una representación de bajo rango de esa matriz. Esto es particularmente útil para simplificar modelos de red neuronal para su despliegue en dispositivos con recursos limitados.\n",
        "\n",
        "Más especificamente, LoRA es una técnica diseñada para modificar modelos de redes neuronales pre-entrenados sin tener que reentrenar todo el modelo. Funciona introduciendo módulos \"aprendibles\", que pueden considerarse como \"bloques LoRA\", en el modelo. Estos bloques LoRA aplican esencialmente una forma truncada de la Descomposición en Valores Singulares (SVD). En otras palabras, LoRA mantiene los pesos principales del modelo pre-entrenado congelados y solo introduce algunos módulos de SVD truncados (los bloques LoRA) en el modelo."
      ],
      "metadata": {
        "id": "KL4SzsvTLMWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRAParametrization(nn.Module):\n",
        "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
        "        super().__init__()\n",
        "        # nos basamos en la sección 4.1 del paper:\n",
        "        #  Usamos una inicalización gaussiana para A y cero para B, donde ∆W = BA es cero al comienzo del entreinamiento\n",
        "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
        "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
        "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
        "\n",
        "        # en la misma sección\n",
        "        #   se escala ∆Wx por α/r , donde α es una constante en r.\n",
        "        #   cuando optimizamos usando Adam, hacer tuning a α es practicamente lo mismo que hacer tuning al learning rate si escalamos  la inicialización.\n",
        "        #   Entonces ponemos α al primer r que encontramos y no tuneamos.\n",
        "        #   La escala ayuda a reducir la necesiadd de retunear los hyperparametros cuando variamos r.\n",
        "        self.scale = alpha / rank\n",
        "        self.enabled = True\n",
        "\n",
        "    def forward(self, original_weights):\n",
        "        if self.enabled:\n",
        "            # Return W + (B*A)*scale\n",
        "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
        "        else:\n",
        "            return original_weights"
      ],
      "metadata": {
        "id": "SZ_gYKLUyopi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.parametrize as parametrize\n",
        "\n",
        "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
        "    # solo añadimos la parametrización a la matriz de pesos, ignoramos el sesgo\n",
        "\n",
        "    # cogemos la sección 4.2 del paper:\n",
        "    #  Solo vamos estudiar los attention weights\n",
        "\n",
        "    features_in, features_out = layer.weight.shape\n",
        "    return LoRAParametrization(\n",
        "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
        "    )\n",
        "\n",
        "parametrize.register_parametrization(\n",
        "    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n",
        ")\n",
        "parametrize.register_parametrization(\n",
        "    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n",
        ")\n",
        "parametrize.register_parametrization(\n",
        "    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n",
        ")\n",
        "\n",
        "\n",
        "def enable_disable_lora(enabled=True):\n",
        "    for layer in [net.linear1, net.linear2, net.linear3]:\n",
        "        layer.parametrizations[\"weight\"][0].enabled = enabled"
      ],
      "metadata": {
        "id": "vlmun0d0yqTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_parameters_lora = 0\n",
        "total_parameters_non_lora = 0\n",
        "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
        "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
        "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(\n",
        "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
        "    )\n",
        "# Los parámetros non-LoRA tienen que matchear con los originales\n",
        "assert total_parameters_non_lora == total_parameters_original\n",
        "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
        "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
        "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
        "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
        "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3OOMPfyyq3K",
        "outputId": "8bffa5fc-e340-422e-abe7-5f1da650c442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n",
            "Total number of parameters (original): 2,807,010\n",
            "Total number of parameters (original + LoRA): 2,813,804\n",
            "Parameters introduced by LoRA: 6,794\n",
            "Parameters incremment: 0.242%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# congelamos los parámetros no-Lora\n",
        "for name, param in net.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        print(f'Freezing non-LoRA parameter {name}')\n",
        "        param.requires_grad = False\n",
        "\n",
        "# cargamos el mnist otra vez pero solo con el 9 (fine-tuning)\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "exclude_indices = mnist_trainset.targets == 9\n",
        "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
        "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
        "# creamos un dataloader para el training\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "# entrenamos la red con LoRA solo en el dígito 9 y solo en 100 batches\n",
        "train(train_loader, net, epochs=1, total_iterations_limit=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eubDvk1QMAgQ",
        "outputId": "0b9688ac-8b5d-4598-8236-c2376b38da3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing non-LoRA parameter linear1.bias\n",
            "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear2.bias\n",
            "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear3.bias\n",
            "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  99%|█████████▉| 99/100 [00:03<00:00, 24.77it/s, loss=0.102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chequeamos si los parámetros originales no han sido modificados por el fine-tuning\n",
        "assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
        "assert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
        "assert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n",
        "\n",
        "enable_disable_lora(enabled=True)\n",
        "# el nuevo peso linear1.weight se obtiene a través de la función \"forward\" de la parametrizacion LoRA\n",
        "# los pesos originales se mueven a net.linear1.parametrizations.weight.original\n",
        "# información de aquí: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n",
        "assert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n",
        "\n",
        "enable_disable_lora(enabled=False)\n",
        "# si quitamos el LoRa, el linear1.weight es el original\n",
        "assert torch.equal(net.linear1.weight, original_weights['linear1.weight'])"
      ],
      "metadata": {
        "id": "QzXMu2beMBSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados:\n",
        "\n",
        "podemos ver que aplicando LoRA mejoramos considerablemente la clasificación. El hiperparámetro  r juega un papel crucial en LoRA, determinando el rango de las matrices de baja dimensionalidad utilizadas para la adaptación. Un valor de\n",
        "r más pequeño simplifica la matriz de baja dimensionalidad, acelerando el entrenamiento, pero puede reducir la calidad de la adaptación y aumentar el riesgo de un ajuste insuficiente. Por otro lado, un valor de  r más alto aumenta la complejidad pero mejora la capacidad del modelo para capturar información específica de la tarea. Encontrar el equilibrio adecuado experimentando con diferentes valores de  r es fundamental para lograr un rendimiento óptimo en nuevas tareas."
      ],
      "metadata": {
        "id": "l1GsQISkMz1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testeamos con LoRa\n",
        "enable_disable_lora(enabled=True)\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjACYkrCyuBd",
        "outputId": "e5029e5e-c8c7-4542-efff-add75b55533a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:14<00:00, 68.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.924\n",
            "wrong counts for the digit 0: 47\n",
            "wrong counts for the digit 1: 27\n",
            "wrong counts for the digit 2: 65\n",
            "wrong counts for the digit 3: 240\n",
            "wrong counts for the digit 4: 89\n",
            "wrong counts for the digit 5: 32\n",
            "wrong counts for the digit 6: 54\n",
            "wrong counts for the digit 7: 137\n",
            "wrong counts for the digit 8: 61\n",
            "wrong counts for the digit 9: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testeamos sin LoRa\n",
        "#vemos una mejora considerable con Lora\n",
        "enable_disable_lora(enabled=False)\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUjFGqloyv67",
        "outputId": "fce1512c-defb-4468-e7db-1ad311c045e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:06<00:00, 159.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.954\n",
            "wrong counts for the digit 0: 31\n",
            "wrong counts for the digit 1: 17\n",
            "wrong counts for the digit 2: 46\n",
            "wrong counts for the digit 3: 74\n",
            "wrong counts for the digit 4: 29\n",
            "wrong counts for the digit 5: 7\n",
            "wrong counts for the digit 6: 36\n",
            "wrong counts for the digit 7: 80\n",
            "wrong counts for the digit 8: 25\n",
            "wrong counts for the digit 9: 116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}