{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low Rank Adaptation\" presenta una innovación en la adaptación de modelos pre-entrenados, centrándose en la eficiencia de parámetros y la reducción de la necesidad de recursos. Los autores señalan que la afinación de los modelos pre-entrenados de gran tamaño es costosa y consume muchos recursos. Los autores introducen DyLoRA, una adaptación de bajo rango dinámica que aborda las limitaciones de LoRA. DyLoRA entrena bloques de LoRA para un rango de rangos, en lugar de un rango único, y clasifica la representación aprendida en diferentes rangos durante el entrenamiento.\n",
        "\n",
        "Para ellos, los resultados muestran que DyLoRA es al menos 7 veces más rápido que LoRA en entrenamiento, sin comprometer significativamente el rendimiento. Además, DyLoRA funciona bien en un rango mucho más amplio de rangos en comparación con LoRA, porque DyLoRA aborda eficazmente dos problemas en los adaptadores de bajo rango: la selección de rango y la dinámica en tiempo de inferencia, logrando evitar la búsqueda de rangos óptimos en escenarios de la vida real con un rendimiento comparable.\n",
        "\n",
        "DyLoRA no aplica SVD, pero, LoRA si. De hecho, al \"congelar\" los demás datos y construir bloques usando SVD, LoRA hace que sean eficientes en términos de parámetros, pero tienen dos problemas principales:\n",
        "\n",
        "Tamaño Fijo de los Bloques: El tamaño de estos bloques es fijo y no se puede modificar después del entrenamiento. Por ejemplo, si necesitamos cambiar el rango de los bloques LoRA, tendríamos que reentrenarlos desde cero.\n",
        "\n",
        "Optimización del Rango: Optimizar el rango de estos bloques requiere una búsqueda exhaustiva y un esfuerzo considerable.\n",
        "\n",
        "DyLoRA, o Adaptación de Bajo Rango Dinámica, aborda estos dos problemas. A diferencia de LoRA, que utiliza una versión truncada y fija de SVD, DyLoRA introduce una forma dinámica de adaptación de bajo rango. Esto significa que en lugar de tener bloques con un tamaño y rango fijos, DyLoRA permite que estos aspectos sean flexibles y se ajusten dinámicamente. Esto puede facilitar la adaptación del modelo a diferentes tareas o datos sin necesidad de reentrenar desde cero y simplifica el proceso de encontrar el rango óptimo para los bloques LoRA.\n",
        "\n",
        "Para utilizar DyLoRA solo hemos adaptado la clase y la parametrización de los pesos.\n",
        "\n",
        "\n",
        "\n",
        "https://neurips2022-enlsp.github.io/papers/paper_37.pdf"
      ],
      "metadata": {
        "id": "WJsjQy7JIrbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importamos las librerías necesarias\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "FDtUUbLlfDXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hacemos torch determinístico\n",
        "_ = torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "oakNS3zmg89J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# cargamos el dataset MNIST para train\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# creamos un dataloader para train\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "# cargamos MNIST para test\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
        "\n",
        "# definimos el device que vamos utilizar\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "E2RIogkznXKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos una red neuronal demasiado ineficiente para clasificar dígitos MNIST\n",
        "\n",
        "class desperdicio(nn.Module):\n",
        "    def __init__(self, hidden_size_1=1000, hidden_size_2=2000):\n",
        "        super(desperdicio,self).__init__()\n",
        "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
        "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = img.view(-1, 28*28)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "net = desperdicio().to(device)"
      ],
      "metadata": {
        "id": "teUtOdJhorP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, net, epochs=5, total_iterations_limit=None):\n",
        "    cross_el = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    total_iterations = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "\n",
        "        loss_sum = 0\n",
        "        num_iterations = 0\n",
        "\n",
        "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
        "        if total_iterations_limit is not None:\n",
        "            data_iterator.total = total_iterations_limit\n",
        "        for data in data_iterator:\n",
        "            num_iterations += 1\n",
        "            total_iterations += 1\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = net(x.view(-1, 28*28))\n",
        "            loss = cross_el(output, y)\n",
        "            loss_sum += loss.item()\n",
        "            avg_loss = loss_sum / num_iterations\n",
        "            data_iterator.set_postfix(loss=avg_loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
        "                return\n",
        "\n",
        "train(train_loader, net, epochs=1)"
      ],
      "metadata": {
        "id": "RkjjVJSBycGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6404844e-fff2-4f61-eae5-5883479d50ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 6000/6000 [08:11<00:00, 12.22it/s, loss=0.236]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#salvamos los pesos originales para poder recuperarlos\n",
        "original_weights = {}\n",
        "for name, param in net.named_parameters():\n",
        "    original_weights[name] = param.clone().detach()"
      ],
      "metadata": {
        "id": "KVq2rBhRyhaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hacemos test en las clasificaciones donde vemos que se equivoca mucho en el dígito 9 en relación a los demás\n",
        "def test():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    wrong_counts = [0 for i in range(10)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, desc='Testing'):\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            output = net(x.view(-1, 784))\n",
        "            for idx, i in enumerate(output):\n",
        "                if torch.argmax(i) == y[idx]:\n",
        "                    correct +=1\n",
        "                else:\n",
        "                    wrong_counts[y[idx]] +=1\n",
        "                total +=1\n",
        "    print(f'Accuracy: {round(correct/total, 3)}')\n",
        "    for i in range(len(wrong_counts)):\n",
        "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6XIFfZSyjl9",
        "outputId": "be5114d9-68de-49d9-a9cd-6ddc9b130fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:06<00:00, 164.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.954\n",
            "wrong counts for the digit 0: 31\n",
            "wrong counts for the digit 1: 17\n",
            "wrong counts for the digit 2: 46\n",
            "wrong counts for the digit 3: 74\n",
            "wrong counts for the digit 4: 29\n",
            "wrong counts for the digit 5: 7\n",
            "wrong counts for the digit 6: 36\n",
            "wrong counts for the digit 7: 80\n",
            "wrong counts for the digit 8: 25\n",
            "wrong counts for the digit 9: 116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# salvamos los parámetros originales\n",
        "total_parameters_original = 0\n",
        "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
        "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
        "print(f'Total number of parameters: {total_parameters_original:,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsjHsT9bynKE",
        "outputId": "c403d601-7f0e-4847-b6c9-8205967042c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
            "Total number of parameters: 2,807,010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DyLoRA permite ajustar dinámicamente tanto el rango (r) como el valor alfa (α), lo que significa que estos parámetros pueden cambiar durante el entrenamiento para adaptarse mejor a la tarea en curso. En contraste, LoRA fija estos valores y no los ajusta durante el entrenamiento.\n",
        "\n",
        "En la implementación de DyLoRA, la función adjust_rank_and_alpha permite cambiar el rango (r) y el valor alfa (α) de manera dinámica y reinicializar los parámetros correspondientes. Esto significa que se puede experimentar con diferentes valores de rango y alfa durante el entrenamiento para encontrar la configuración óptima."
      ],
      "metadata": {
        "id": "y1owBosEU4w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#aquí hacemos el cambio de LoRA a DyLoRA\n",
        "class DynamicLoRAParametrization(nn.Module):\n",
        "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
        "        super(DynamicLoRAParametrization, self).__init__()\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.features_in = features_in\n",
        "        self.features_out = features_out\n",
        "        self.device = device\n",
        "        self.lora_A = nn.Parameter(torch.zeros((rank, features_out)).to(device))\n",
        "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
        "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
        "        self.scale = self.alpha / self.rank\n",
        "\n",
        "    def forward(self, original_weights):\n",
        "        # Return W + scaled(BA)\n",
        "        delta_W = torch.matmul(self.lora_B, self.lora_A) * self.scale\n",
        "        return original_weights + delta_W.view(original_weights.shape)\n",
        "\n",
        "    def adjust_rank_and_alpha(self, new_rank, new_alpha):\n",
        "        # Dynamically adjust `rank` and `alpha`, and reinitialize parameters\n",
        "        self.rank = new_rank\n",
        "        self.alpha = new_alpha\n",
        "        self.lora_A = nn.Parameter(torch.zeros((self.rank, self.features_out)).to(self.device))\n",
        "        self.lora_B = nn.Parameter(torch.zeros((self.features_in, self.rank)).to(self.device))\n",
        "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
        "        self.scale = self.alpha / self.rank"
      ],
      "metadata": {
        "id": "SZ_gYKLUyopi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.parametrize as parametrize\n",
        "\n",
        "\n",
        "def linear_layer_parameterization_dynamic(layer, device, rank=1, lora_alpha=1):\n",
        "    # solo añadimos la parametrización a la matriz de pesos, ignoramos el sesgo\n",
        "\n",
        "    # Utilizamos la versión dinámica de LoRAParametrization\n",
        "    features_in, features_out = layer.weight.shape\n",
        "    return DynamicLoRAParametrization(\n",
        "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
        "    )\n",
        "\n",
        "# Registramos la parametrización dinámica para las capas\n",
        "parametrize.register_parametrization(\n",
        "    net.linear1, \"weight\", linear_layer_parameterization_dynamic(net.linear1, device)\n",
        ")\n",
        "parametrize.register_parametrization(\n",
        "    net.linear2, \"weight\", linear_layer_parameterization_dynamic(net.linear2, device)\n",
        ")\n",
        "parametrize.register_parametrization(\n",
        "    net.linear3, \"weight\", linear_layer_parameterization_dynamic(net.linear3, device)\n",
        ")\n",
        "\n",
        "def enable_disable_lora_dynamic(enabled=True):\n",
        "    for layer in [net.linear1, net.linear2, net.linear3]:\n",
        "        layer.parametrizations.weight[0].enabled = enabled"
      ],
      "metadata": {
        "id": "vlmun0d0yqTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplemente calculamos el total en este contexto\n",
        "total_parameters = sum(p.numel() for p in net.parameters())\n",
        "print(f'Total number of parameters (including LoRA): {total_parameters:,}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3OOMPfyyq3K",
        "outputId": "f48f9566-733b-4cc8-e387-a29f9cc3ed34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters (including LoRA): 2,813,804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# congelamos los parámetros no-Lora\n",
        "for name, param in net.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        print(f'Freezing non-LoRA parameter {name}')\n",
        "        param.requires_grad = False\n",
        "\n",
        "# cargamos el mnist otra vez pero solo con el 9 (fine-tuning)\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "exclude_indices = mnist_trainset.targets == 9\n",
        "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
        "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
        "# creamos un dataloader para el training\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "# entrenamos la red con DyLoRA solo en el dígito 9 y solo en 100 batches\n",
        "train(train_loader, net, epochs=1, total_iterations_limit=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eubDvk1QMAgQ",
        "outputId": "f75da10d-42f1-489c-99c7-7d6bac4f7934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing non-LoRA parameter linear1.bias\n",
            "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear2.bias\n",
            "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear3.bias\n",
            "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  99%|█████████▉| 99/100 [00:04<00:00, 21.27it/s, loss=0.0265]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "full_test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(root='./data', train=False, download=True, transform=transform),\n",
        "    batch_size=10,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "correct_predictions_9 = 0\n",
        "total_samples_9 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in full_test_loader:\n",
        "\n",
        "        digit_9_indices = labels == 9\n",
        "        images_9 = images[digit_9_indices]\n",
        "        labels_9 = labels[digit_9_indices]\n",
        "\n",
        "        if len(images_9) > 0:\n",
        "            outputs = net(images_9)  # pasamos nuestro modelo tuneado\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions_9 += (predicted == labels_9).sum().item()\n",
        "            total_samples_9 += labels_9.size(0)\n",
        "\n",
        "\n",
        "accuracy_9 = (correct_predictions_9 / total_samples_9) * 100\n",
        "print(f'Accuracy on digit 9 using fine-tuned model: {accuracy_9:.2f}%')\n"
      ],
      "metadata": {
        "id": "QzXMu2beMBSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d7152b-5825-43f4-b149-dd38f95997ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on digit 9 using fine-tuned model: 99.90%\n"
          ]
        }
      ]
    }
  ]
}